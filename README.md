ğŸš€ Custom ELT Project

This repository demonstrates a modern Extract, Load, Transform (ELT) pipeline using a real-world data engineering stack. The project leverages Docker, Airflow, Airbyte, dbt, cron, PostgreSQL, and Python scripting to orchestrate ingestion, transformation, and automation.

ğŸ“‚ Repository Structure

docker-compose.yaml â€“ Orchestrates all services (Airflow, Airbyte, PostgreSQL, etc.)

airflow/ â€“ Contains DAGs and supporting files for orchestrating the ELT pipeline

dags/elt_pipeline_dag.py â€“ Airflow DAG defining the pipeline workflow

airbyte/ â€“ Configuration for data ingestion (e.g., syncing source PostgreSQL â†’ destination PostgreSQL)

dbt_project/ â€“ dbt models, seeds, and configs for SQL-based transformations

scripts/ â€“ Custom Python or cron jobs to support extra automation or data checks

source_db_init/init.sql â€“ Initializes the source PostgreSQL database with sample tables and data

âš™ï¸ Tools Used

Docker & Docker Compose â†’ Containerize and run all services together

PostgreSQL â†’ Used as both source and destination databases

Airbyte â†’ Handles Extract + Load (syncs data from source PostgreSQL â†’ destination PostgreSQL)

dbt â†’ Handles Transform (runs SQL models inside the destination warehouse)

Airflow â†’ Orchestrates the pipeline (e.g., trigger Airbyte sync â†’ run dbt models)

cron â†’ Lightweight job scheduling for recurring scripts or health checks

Python â†’ Custom scripting for automation or data validation

ğŸ”„ How It Works

Source Database Initialization

The source_postgres container is populated with sample data (tables, users, films, categories, etc.) from init.sql.

Data Ingestion with Airbyte

Airbyte is configured to extract data from source_postgres and load it into destination_postgres.

Pipeline Orchestration with Airflow

Airflow DAG (elt_pipeline_dag.py) orchestrates:

Trigger Airbyte sync (Extract + Load)

Run dbt models (Transform)

Optionally run Python validation scripts

Transformations with dbt

dbt models clean and transform the raw data into analytics-ready tables.

Scheduling

Pipelines can be scheduled via Airflow or cron jobs for recurring runs.

ğŸš€ Getting Started
Prerequisites

Docker & Docker Compose installed on your machine

Setup
# Clone repository
git clone https://github.com/your-username/custom-elt-project.git
cd custom-elt-project

# Start all services
docker-compose up

Accessing Services

Airflow UI â†’ http://localhost:8080

Airbyte UI â†’ http://localhost:8000

Source PostgreSQL â†’ port 5433

Destination PostgreSQL â†’ port 5434

âœ… Workflow Summary

Airbyte extracts data from source_postgres

Airbyte loads raw data into destination_postgres

Airflow triggers the pipeline and orchestrates tasks

dbt runs SQL models to transform the raw data into clean, analytics-ready tables

Python scripts / cron jobs handle any extra automation or checks

ğŸ“Š Example Use Case

Imagine a movie database system:

Source DB contains raw user, film, and actor tables.

Airbyte syncs this into the destination DB.

dbt creates transformed tables like dim_users, dim_films, and fact_user_activity.

Airflow ensures everything runs in the right order daily at midnight.